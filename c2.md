tensor - data container 
tensor operations - represented by layer

Neural networks consist entirely of chains of tensor operations which are simple geometric transformations of the input data.

You can interpret a neural network as a very complex geometric transformation in a high-dimensional space implemented via a series of simple steps.

gradient descent - how to adjust weights and by how much

derivative - 

continuity - a small change in X can only result in a small change in Y.

f(x + eplison_x) = y + a * eplison_x

- valid only when x is close to p
- slope a is called the derivative of f in p

For every differentiable function f(x), there exists a drivation function f'(x) that maps values x to the slope of the local linear approximation of f in those points.

x -> slope 


gradient = derivative of a tensor operation 

Gradients are just the generalization of the concept of derivatives to functions that take tensors as inputs.

Partial derivatives 

grad_ij(f(W), w_ij) = derivative of loss_value = f(W) with respect to the coefficient W[i, j], assuming all other coefficients are constant.

grad_ij is called the partial derivative of f with respect to W[i, j]


W1 = W0 - step * grad(f(W0), W0)


Intractable to do grad(f(W), W) = 0 for neural networks with thousands/millions of params.

W -= learning_rate * gradient 

Mini-batch stochastic gradient descent = mini-batch SGD

Stochastic = each batch of data is drawn at random
